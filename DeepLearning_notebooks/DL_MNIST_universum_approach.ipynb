{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Classification\n",
    "\n",
    "Maria Leonor Zamora\n",
    "\n",
    "***\n",
    "***\n",
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import pickle \n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded\n",
      "3000\n",
      "750\n"
     ]
    }
   ],
   "source": [
    "# This creates the data folder -> raw and processed\n",
    "import sys\n",
    "sys.path.append(\"script\")\n",
    "from sub import subMNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"DL_A1_LargeNet\")\n",
    "import image_transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pickle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainset_imoprt = pickle.load(open(\"A1_deep_data/train_labeled.p\", \"rb\"))\n",
    "trainset_unlabel_imoprt = pickle.load(open(\"A1_deep_data/train_unlabeled_1.p\", \"rb\"))\n",
    "validset_import = pickle.load(open(\"A1_deep_data/validation.p\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 28, 28)\n",
      "(47000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_NUM_SIZE = 64\n",
    "MOMENTUM_VALUE = 0.9\n",
    "\n",
    "trainset_imoprt_numpy = trainset_imoprt.train_data.numpy()\n",
    "trainset_labels_imoprt_numpy = trainset_imoprt.train_labels.numpy()\n",
    "\n",
    "trainset_unlabel_imoprt_numpy = trainset_unlabel_imoprt.train_data.numpy()\n",
    "\n",
    "validset_import_numpy =validset_import.test_data.numpy()\n",
    "validset_labels_import_numpy = validset_import.test_labels.numpy()\n",
    "\n",
    "print ( trainset_imoprt.train_data.numpy().shape )\n",
    "print ( trainset_unlabel_imoprt.train_data.numpy().shape )\n",
    "print ( validset_import.test_data.numpy().shape )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def range_scale():\n",
    "    # 0.8 to 1.2\n",
    "    return 0.8\n",
    "\n",
    "def range_rotation():\n",
    "    # -pi/6 to pi/6\n",
    "    return 45.\n",
    "\n",
    "def random_translation():\n",
    "    xrand = np.random\n",
    "    if xrand <= 0.5: return True\n",
    "    else: return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11ecdd150>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDwRF+IGg/g9u3HZr7xcc8gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ee71fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter( trainset_imoprt_numpy[0:1] )\n",
    "images = dataiter.next()\n",
    "plt.imshow(images)\n",
    "#plt.imshow(images, cmap='gray')\n",
    "#print(' '.join('%5s'%train_label_numpy[j]for j in range(BATCH_NUM_SIZE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TRAIN - TRANSFORMATION\n",
    "trainset_scaled = np.array([image_transformations.scale(x,range_scale()) for x in trainset_imoprt_numpy ])\n",
    "trainset_rotated = np.array([image_transformations.rotation(x,range_rotation()) for x in trainset_scaled ])\n",
    "trainset_translated = np.array([image_transformations.translation(x,0.5,horizontal=random_translation()) for x in trainset_rotated ])\n",
    "\n",
    "trainset_loader_preprocessed = torch.from_numpy(trainset_translated)\n",
    "trainset_imoprt.train_data = trainset_loader_preprocessed.clone()\n",
    "trainset_imoprt.train_labels = torch.from_numpy(trainset_labels_imoprt_numpy).clone()\n",
    "\n",
    "\n",
    "# UNLABELED TRAIN - TRANSFORMATION\n",
    "trainset_unlabel_scaled = np.array([image_transformations.scale(x,range_scale()) for x in trainset_unlabel_imoprt_numpy ])\n",
    "trainset_unlabel_rotated = np.array([image_transformations.rotation(x,range_rotation()) for x in trainset_unlabel_scaled])\n",
    "trainset_unlabel_translated = np.array([image_transformations.translation(x,0.5,horizontal=random_translation()) for x in trainset_unlabel_rotated])\n",
    "\n",
    "trainset_unlabel_loader_preprocessed = torch.from_numpy(trainset_unlabel_translated)\n",
    "trainset_unlabel_imoprt.train_data = trainset_unlabel_loader_preprocessed.clone()\n",
    "trainset_unlabel_imoprt.train_labels = torch.from_numpy(np.repeat(-1,\n",
    "                                                trainset_unlabel_imoprt.train_data.size()[0])).clone()\n",
    "\n",
    "# TEST - TRANSFORMATION\n",
    "validset_scaled = np.array([image_transformations.scale(x,range_scale()) for x in validset_import_numpy ])\n",
    "validset_rotated = np.array([image_transformations.rotation(x,range_rotation()) for x in trainset_scaled ])\n",
    "validset_translated = np.array([image_transformations.translation(x,0.5,horizontal=random_translation()) for x in trainset_rotated ])\n",
    "\n",
    "validset_loader_preprocessed = torch.from_numpy(validset_translated)\n",
    "validset_import.train_data = validset_loader_preprocessed.clone()\n",
    "validset_import.train_labels = torch.from_numpy(validset_labels_import_numpy).clone()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataiter = iter( trainset_translated[0:1] )\n",
    "images = dataiter.next()\n",
    "plt.imshow(images)\n",
    "#plt.imshow(images, cmap='gray')\n",
    "#print(' '.join('%5s'%train_label_numpy[j]for j in range(BATCH_NUM_SIZE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model \n",
    "\n",
    "21-layer convolutional network (ConvNet) , in which the inputs are 32-by-32 images and all convolutional layers are 3-by-3 and fully padded. ll pooling layers are max-pooling, and ReLUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Conv2d has stride=1 default where pixel = ‘stride’\n",
    "        # Conv1d sounds, conv2d images, conv3d videos\n",
    "        # input size (N,Cin,H,W)(N,Cin,H,W) and output (N,Cout,Hout,Wout)(N,Cout,Hout,Wout)\n",
    "        # out(Ni,Coutj)=bias(Coutj)+∑Cin−1k=0weight(Coutj,k)⋆input(Ni,k)\n",
    "        \n",
    "        TOTAL_KERNEL = 3\n",
    "        padding=TOTAL_PADDING = 3\n",
    "        channel1 = 10\n",
    "        channel2 = 10\n",
    "        channel3 = 10\n",
    "        channel4 = 10\n",
    "        self.last_channel = channel4\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, channel1, kernel_size=TOTAL_KERNEL, padding=TOTAL_PADDING)\n",
    "        self.conv2 = nn.Conv2d(channel1,channel1,kernel_size=TOTAL_KERNEL, padding=TOTAL_PADDING)\n",
    "        self.conv3 = nn.Conv2d(channel1,channel1,kernel_size=TOTAL_KERNEL, padding=TOTAL_PADDING)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(channel1,channel2, kernel_size=TOTAL_KERNEL, padding=TOTAL_PADDING)\n",
    "        self.conv6 = nn.Conv2d(channel2,channel2,kernel_size=TOTAL_KERNEL, padding=TOTAL_PADDING)\n",
    "        self.conv7 = nn.Conv2d(channel2,channel2,kernel_size=TOTAL_KERNEL, padding=TOTAL_PADDING)\n",
    "        self.conv8 = nn.Conv2d(channel2,channel2,kernel_size=TOTAL_KERNEL, padding=TOTAL_PADDING)\n",
    "        \n",
    "        self.conv10 = nn.Conv2d(channel2,channel3, kernel_size=TOTAL_KERNEL, padding=TOTAL_PADDING)\n",
    "        self.conv11 = nn.Conv2d(channel3,channel3,kernel_size=TOTAL_KERNEL, padding=TOTAL_PADDING)\n",
    "        self.conv12 = nn.Conv2d(channel3,channel3,kernel_size=TOTAL_KERNEL, padding=TOTAL_PADDING)\n",
    "        self.conv13 = nn.Conv2d(channel3,channel3,kernel_size=TOTAL_KERNEL, padding=TOTAL_PADDING)\n",
    "        \n",
    "        self.conv15 = nn.Conv2d(channel3,channel3, kernel_size=TOTAL_KERNEL, padding=TOTAL_PADDING)\n",
    "        self.conv16 = nn.Conv2d(channel3,channel3,kernel_size=TOTAL_KERNEL, padding=TOTAL_PADDING)\n",
    "        self.conv17 = nn.Conv2d(channel3,channel3,kernel_size=TOTAL_KERNEL, padding=TOTAL_PADDING)\n",
    "        self.conv18 = nn.Conv2d(channel3,channel3,kernel_size=TOTAL_KERNEL, padding=TOTAL_PADDING)\n",
    "        \n",
    "        self.conv20 = nn.Conv2d(channel3,channel4, kernel_size=TOTAL_KERNEL, padding=TOTAL_PADDING)\n",
    "        self.conv21 = nn.Conv2d(channel4,channel4,kernel_size=TOTAL_KERNEL, padding=TOTAL_PADDING)\n",
    "        self.conv22 = nn.Conv2d(channel4,channel4,kernel_size=TOTAL_KERNEL, padding=TOTAL_PADDING)\n",
    "        self.conv23 = nn.Conv2d(channel4,channel4,kernel_size=TOTAL_KERNEL, padding=TOTAL_PADDING)\n",
    "        \n",
    "        self.conv_drop_a = nn.Dropout2d()\n",
    "        \n",
    "        self.fc1 = nn.Linear(channel4*16*16, 50)    # LAST OUTPUT LAYER x LAST SIZE x LAST SIZE :  AFTER LAST CONV2D\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #print (\"original size = \",x.size())\n",
    "        #x = self.conv1(x)\n",
    "        #print (\"conv size = \", x.size())\n",
    "        #x = F.max_pool2d(self.conv1(x), 2)\n",
    "        #print (\"max pool size = \", x.size())\n",
    "        #x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        #print (\"relu size 1\", x.size())\n",
    "        \n",
    "        x = self.conv3( self.conv2( self.conv1(x) ) )\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        x = self.conv8( self.conv7( self.conv6( self.conv5(x) ) ) )\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        x = self.conv13( self.conv12( self.conv11( self.conv10(x) ) ) )\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        x = self.conv18( self.conv17( self.conv16( self.conv15(x) ) ) )\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        x = self.conv23( self.conv22( self.conv21( self.conv20(x) ) ) )\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        #print (x.size())\n",
    "        \n",
    "        #x = F.relu(F.max_pool2d(self.conv_drop_a(self.conv18(x)), 2))\n",
    "        \n",
    "        x = x.view(-1, self.last_channel*16*16)\n",
    "        #print (\"before drop\", x.size())\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        #print (\"after drop\", x.size())\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        #print (\"after drop\", x.size())\n",
    "        \n",
    "        return F.log_softmax(x)\n",
    "\n",
    "model = Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=MOMENTUM_VALUE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CPU only training\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test(epoch, valid_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target).data[0]\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(valid_loader) # loss function already averages over batch size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(valid_loader.dataset),\n",
    "        100. * correct / len(valid_loader.dataset)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset_imoprt, batch_size=BATCH_NUM_SIZE, shuffle=True)\n",
    "#train_unlabel_loader = torch.utils.data.DataLoader(trainset_unlabel_imoprt, batch_size=BATCH_NUM_SIZE, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(validset_import, batch_size=BATCH_NUM_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/3000 (0%)]\tLoss: 2.303890\n",
      "Train Epoch: 1 [640/3000 (21%)]\tLoss: 2.298603\n",
      "Train Epoch: 1 [1280/3000 (43%)]\tLoss: 2.307538\n",
      "Train Epoch: 1 [1920/3000 (64%)]\tLoss: 2.300804\n",
      "Train Epoch: 1 [2560/3000 (85%)]\tLoss: 2.306649\n",
      "\n",
      "Test set: Average loss: 2.3027, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Train Epoch: 2 [0/3000 (0%)]\tLoss: 2.300977\n",
      "Train Epoch: 2 [640/3000 (21%)]\tLoss: 2.306214\n",
      "Train Epoch: 2 [1280/3000 (43%)]\tLoss: 2.301062\n",
      "Train Epoch: 2 [1920/3000 (64%)]\tLoss: 2.307528\n",
      "Train Epoch: 2 [2560/3000 (85%)]\tLoss: 2.299569\n",
      "\n",
      "Test set: Average loss: 2.3027, Accuracy: 1000/10000 (10%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(1, 3):\n",
    "    train(epoch)\n",
    "    test(epoch, valid_loader)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
