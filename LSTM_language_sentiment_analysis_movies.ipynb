{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM lanugage model - Fall 2016\n",
    "\n",
    "The data used here is a treebank dataset for sentiment analysis ( based on the sentence structure ) available in [Stanford's website](https://nlp.stanford.edu/sentiment/). The texts are basically movie reviews from [Rotten Tomatoes](http://rottentomatoes.com) originally collected and published by Pang and Lee (2005), but this dataset has labels for every syntactically plausible phrase in thousands of sentences. Half of sentences were considered positive and the other half negative and labels were extracted from longer movie review. \n",
    "\n",
    "The [Stanford Parser](https://nlp.stanford.edu/software/lex-parser.shtml) was used as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Import the data from txt files **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Prof Bowman Class exercise - Random Seed used!!! (see below)\n",
    "\n",
    "def load_sst_data(path):\n",
    "    \n",
    "    import re\n",
    "    import random\n",
    "    \n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)  # Strip out the parse information \n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "            \n",
    "    print path\n",
    "    print \"\\n ----> Line example in the tree-bank format\\n\", line\n",
    "    sentiment_labels = [x.replace(\"(\",'').replace(\")\",'') for x in re.findall('\\(\\d\\s[\\w]*\\)',line)]\n",
    "    print \" ----> Line example sentiment format\\n\", sentiment_labels,\"\\n\"\n",
    "    print \" ----> Line example full text format\\n\", text,\"\\n\"\n",
    "            \n",
    "    random.seed(1)\n",
    "    random.shuffle(data)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{0: 'very negative', 1: 'negative', 2: 'None', 3: 'positive', 4: 'very positive'} \n",
      "\n",
      "data/trees/train.txt\n",
      "\n",
      " ----> Line example in the tree-bank format\n",
      "(1 (2 (2 In) (2 this)) (1 (2 case) (1 (1 zero) (2 .))))\n",
      "\n",
      " ----> Line example sentiment format\n",
      "['2 In', '2 this', '2 case', '1 zero'] \n",
      "\n",
      " ----> Line example full text format\n",
      " In this case zero . \n",
      "\n",
      "data/trees/dev.txt\n",
      "\n",
      " ----> Line example in the tree-bank format\n",
      "(1 (1 (1 (0 (1 (2 Schaeffer) (0 (2 has) (0 (2 to) (0 (2 (2 find) (2 (2 some) (2 hook))) (0 (2 on) (1 (2 which) (1 (2 to) (0 (2 hang) (0 (2 his) (1 (1 (3 persistently) (1 useless)) (2 movies))))))))))) (2 ,)) (2 and)) (1 (2 it) (2 (2 (2 might) (2 (2 as) (3 well))) (2 (2 be) (2 (2 (2 the) (2 resuscitation)) (2 (2 of) (2 (2 the) (2 (2 middle-aged) (2 character))))))))) (2 .))\n",
      "\n",
      " ----> Line example sentiment format\n",
      "['2 Schaeffer', '2 has', '2 to', '2 find', '2 some', '2 hook', '2 on', '2 which', '2 to', '2 hang', '2 his', '3 persistently', '1 useless', '2 movies', '2 and', '2 it', '2 might', '2 as', '3 well', '2 be', '2 the', '2 resuscitation', '2 of', '2 the', '2 character'] \n",
      "\n",
      " ----> Line example full text format\n",
      " Schaeffer has to find some hook on which to hang his persistently useless movies , and it might as well be the resuscitation of the middle-aged character . \n",
      "\n",
      "data/trees/test.txt\n",
      "\n",
      " ----> Line example in the tree-bank format\n",
      "(0 (2 (1 -LRB-) (2 (2 U) (3 -RRB-))) (0 (0 (2 nrelentingly) (0 stupid)) (2 .)))\n",
      "\n",
      " ----> Line example sentiment format\n",
      "['2 U', '2 nrelentingly', '0 stupid'] \n",
      "\n",
      " ----> Line example full text format\n",
      " -LRB- U -RRB- nrelentingly stupid . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Positive (3,4) and negative (0,1) to understand tree bank\n",
    "easy_label_map = {0:0, 1:0, 2:None, 3:1, 4:1}\n",
    "easy_label_map = {0:'very negative', 1:'negative', 2:'None', 3:'positive', 4:'very positive'}\n",
    "print \"\\n\", easy_label_map,\"\\n\"\n",
    "    \n",
    "data_home = 'data/trees'\n",
    "\n",
    "training_set = load_sst_data(data_home  + '/train.txt')\n",
    "dev_set = load_sst_data(data_home  + '/dev.txt')\n",
    "test_set = load_sst_data(data_home  + '/test.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of one review\n",
      "\n",
      "{'text': 'Beneath the uncanny , inevitable and seemingly shrewd facade of movie-biz farce ... lies a plot cobbled together from largely flat and uncreative moments .'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print \"Example of one review\\n\\n\", training_set[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Convert the data to index vectors**\n",
    "\n",
    "To simplify implementation I am using length of 15 words per sentence. I'll use the mark of the start of each sentence with < S>, end with </ S>, mark tokens after </S> with a special word symbol < PAD>, and out-of-vocabulary words with < UNK>, for unknown. \n",
    "\n",
    "This vocabulary is constructed based on common words (more than 35 times repeated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size 445\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def sentence_to_padded_index_sequence(datasets):\n",
    "    '''Annotates datasets with feature vectors.'''\n",
    "    \n",
    "    START = \"<S>\"\n",
    "    END = \"</S>\"\n",
    "    END_PADDING = \"<PAD>\"\n",
    "    UNKNOWN = \"<UNK>\"\n",
    "    SEQ_LEN = 16\n",
    "    \n",
    "    # Extract vocabulary\n",
    "    def tokenize(string):\n",
    "        return string.lower().split()\n",
    "    \n",
    "    word_counter = collections.Counter()\n",
    "    for example in datasets[0]:\n",
    "        word_counter.update(tokenize(example['text']))\n",
    "    \n",
    "    vocabulary = set([word for word in word_counter if word_counter[word] > 35])\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary = [START, END, END_PADDING, UNKNOWN] + vocabulary\n",
    "    print \"vocabulary size\", len(vocabulary)\n",
    "        \n",
    "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "    indices_to_words = {v: k for k, v in word_indices.items()}\n",
    "        \n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            example['index_sequence'] = np.zeros((SEQ_LEN), dtype=np.int32)\n",
    "            \n",
    "            token_sequence = [START] + tokenize(example['text']) + [END]\n",
    "            \n",
    "            for i in range(SEQ_LEN):\n",
    "                if i < len(token_sequence):\n",
    "                    if token_sequence[i] in word_indices:  ## Words outside vocab are those with counts less than 25\n",
    "                        index = word_indices[token_sequence[i]]\n",
    "                    else:\n",
    "                        index = word_indices[UNKNOWN]\n",
    "                else:\n",
    "                    index = word_indices[END_PADDING]  ## If there are no more words then <PAD> is the feature written\n",
    "                example['index_sequence'][i] = index\n",
    "    return indices_to_words, word_indices\n",
    "    \n",
    "    \n",
    "indices_to_words, word_indices = sentence_to_padded_index_sequence([training_set, dev_set, test_set])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Beneath the uncanny , inevitable and seemingly shrewd facade of movie-biz farce ... lies a plot cobbled together from largely flat and uncreative moments .', 'index_sequence': array([  0,   3, 334,   3, 126,   3, 291,   3,   3,   3, 383,   3,   3,\n",
      "       350,   3, 109], dtype=int32)}\n",
      "Words in the vocabulary = 445\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print training_set[0]\n",
    "print \"Words in the vocabulary =\", len(word_indices)\n",
    "#print [(k,word_indices[k]) for k in sorted(word_indices,key=word_indices.__getitem__)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM -Language Model **\n",
    "\n",
    "This model will have dropout on the non-recurrent connections. IT Computes sparse softmax cross entropy between logits and labels with \"tf.nn.sparse_softmax_cross_entropy_with_logits\" (measures the probability error in discrete classification tasks in which the classes are mutually exclusive - each entry is in exactly one class). \n",
    "\n",
    "For this case the \"label\" is the next word of a sentence, since **a language model** considers  probability distribution over sequences (sentences) by assigning probability $P(w_{1},... ,w_{m})$ to the whole sequence (relative likelihood).  Language modeling is used for speech recognition, machine translation, part-of-speech tagging, and others. We basically consider that the previous words (context) should be able to predict the new one.\n",
    "\n",
    "$$P(w_{t}|{\\text{context}})\\,\\forall t\\in V$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class LanguageModel:\n",
    "    \n",
    "    def __init__(self, vocab_size, sequence_length):\n",
    "        \n",
    "        # Define the hyperparameters\n",
    "        self.learning_rate = 0.3  # Should be about right\n",
    "        self.training_epochs = 100   # How long to train for - chosen to fit within class time\n",
    "        self.display_epoch_freq = 1  # How often to test and print out statistics\n",
    "        self.dim = 32  # The dimension of the hidden state of the RNN\n",
    "        self.embedding_dim = 16  # The dimension of the learned word embeddings\n",
    "        self.batch_size = 256    # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n",
    "        self.vocab_size = vocab_size  # Defined by the file reader above\n",
    "        self.sequence_length = sequence_length  # Defined by the file reader above\n",
    "        self.keep_rate = 0.75  # Used in dropout (at training time only, not at sampling time)\n",
    "    \n",
    "        # Define the trained parameters.\n",
    "        self.E = tf.Variable(tf.random_normal([self.vocab_size, self.embedding_dim], stddev=0.1))\n",
    "        \n",
    "        self.W = {'f': tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1)),\n",
    "                  'i': tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1)),\n",
    "                  'C': tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1)),\n",
    "                  'o': tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1)),\n",
    "                  'cl': tf.Variable(tf.random_normal([self.dim, vocab_size], stddev=0.1))\n",
    "                  }\n",
    "        \n",
    "        self.b = {'f': tf.Variable(tf.random_normal([self.dim], stddev=0.1)),\n",
    "                  'i': tf.Variable(tf.random_normal([self.dim], stddev=0.1)),\n",
    "                  'C': tf.Variable(tf.random_normal([self.dim], stddev=0.1)),\n",
    "                  'o': tf.Variable(tf.random_normal([self.dim], stddev=0.1)),\n",
    "                  'cl': tf.Variable(tf.random_normal([vocab_size], stddev=0.1))\n",
    "                  }\n",
    "\n",
    "        # Define the input placeholder(s).\n",
    "        self.x = tf.placeholder(tf.int32, [None,self.sequence_length])\n",
    "        self.x_slices = tf.split( self.x,self.sequence_length, axis=1 )   # change 2017\n",
    "        self.keep_rate_ph = tf.placeholder(tf.float32, [])\n",
    "        \n",
    "        # self.logits should contain one [batch_size, vocab_size]-shaped TF tensor of logits \n",
    "        #   for each step of the model.\n",
    "        self.logits = []\n",
    "        # self.costs should contain one [batch_size]-shaped TF tensor of cross-entropy loss \n",
    "        #   values for each step of the model.\n",
    "        self.costs = []\n",
    "        # self.h and c should each start contain one [batch_size, dim]-shaped TF tensor of LSTM\n",
    "        #   activations for each of the lengthnumber+1 *states* of the model -- one tensor of zeros for the \n",
    "        #   starting state followed by one tensor each for the remaining lengthnumber steps.\n",
    "        self.h_zero = tf.zeros( [self.batch_size, self.dim] )   # change 2017\n",
    "        self.c_zero = tf.zeros( [self.batch_size, self.dim] )   # change 2017\n",
    "        self.h = []\n",
    "        self.c =[]\n",
    "                \n",
    "        # Define one step of the RNN for each word-place (index)\n",
    "        def step(x, h_prev, c_prev):\n",
    "            emb_x = tf.nn.embedding_lookup(self.E, x)\n",
    "            emb_x = tf.nn.dropout(emb_x, self.keep_rate_ph)\n",
    "            emb_h_prev = tf.concat(values=[emb_x, h_prev],axis=1)   # change 2017 instead of tf.concat(1,[emb_x, h_prev]) \n",
    "            f_t = tf.nn.sigmoid( tf.matmul(emb_h_prev, self.W['f'])  + self.b['f'] )\n",
    "            i_t = tf.nn.sigmoid( tf.matmul(emb_h_prev, self.W['i'])  + self.b['i'] )\n",
    "            C_tilde_t = tf.nn.tanh( tf.matmul(emb_h_prev, self.W['C'])  + self.b['C'])\n",
    "            C_t = tf.multiply( f_t, c_prev) +  tf.multiply( i_t, C_tilde_t)     # change 2017  instead of \"mul\"\n",
    "            o_t = tf.nn.sigmoid( tf.matmul(emb_h_prev, self.W['o'])  + self.b['o'] )\n",
    "            h = o_t * tf.nn.tanh( C_t )\n",
    "            return h, C_t\n",
    "        \n",
    "        h_prev = self.h_zero\n",
    "        c_prev = self.c_zero\n",
    "        \n",
    "        # Start unrolling the text (sentence), each word will get the same process (step)\n",
    "        for t in range(self.sequence_length-1):\n",
    "            x_t = tf.reshape(self.x_slices[t], [-1])\n",
    "            h_prev, c_prev = step(x_t, h_prev, c_prev)\n",
    "            self.logit_t = tf.matmul(tf.nn.dropout(h_prev, self.keep_rate_ph), self.W['cl']) + self.b['cl']\n",
    "            self.x_t_new = tf.reshape(self.x_slices[t+1], [-1])\n",
    "            self.logits.append( self.logit_t )\n",
    "            self.costs.append(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logit_t, labels=self.x_t_new ))\n",
    "            self.h.append(h_prev)\n",
    "            self.c.append(c_prev)\n",
    "        \n",
    "        \n",
    "        #Sum costs for each word in each example, but average cost across examples.\n",
    "        self.costs_tensor = tf.concat([tf.expand_dims(cost, 1) for cost in self.costs] , 1)\n",
    "        self.cost_per_example = tf.reduce_sum(self.costs_tensor, 1)\n",
    "        self.total_cost = tf.reduce_mean(self.cost_per_example)\n",
    "            \n",
    "        # This library call performs the main SGD update equation\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.total_cost)\n",
    "        \n",
    "        # Create an operation to fill zero values in for W and b\n",
    "        self.init = tf.initialize_all_variables()\n",
    "        \n",
    "        # Create a placeholder for the session that will be shared between training and evaluation\n",
    "        self.sess = None\n",
    "        \n",
    "                \n",
    "    def train(self, training_data):\n",
    "        \n",
    "        def get_minibatch(dataset, start_index, end_index):\n",
    "            indices = range(start_index, end_index)\n",
    "            vectors = np.vstack([dataset[i]['index_sequence'] for i in indices])\n",
    "            return vectors\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.sess.run(self.init)\n",
    "        print 'Training.'\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(self.training_epochs):\n",
    "            random.shuffle(training_set)\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(len(training_set) / self.batch_size)\n",
    "            \n",
    "            # Loop over all batches in epoch\n",
    "            for i in range(total_batch):\n",
    "                # Assemble a minibatch of the next B examples: we ask for the indeex S\n",
    "                # EQUENCE OF EACH OBSERVATION, BASICALLY VECTORS WITH THE NUMBERS (INDEX TO WORDS)\n",
    "                minibatch_vectors = get_minibatch(training_set, self.batch_size * i, self.batch_size * (i + 1))\n",
    "\n",
    "                # Run the optimizer to take a gradient step, and also fetch the value of the \n",
    "                # cost function for logging\n",
    "                \n",
    "                # Example of how I PRINT actual values\n",
    "                #xslices,x_new, logit_t = self.sess.run([self.x_slices, self.x_t_new, self.logit_t ],\n",
    "                #                                       feed_dict={self.x: minibatch_vectors})\n",
    "                #print \"x slices\", xslices[0], \"x new\", x_new, \"logit\", logit_t\n",
    "\n",
    "                _, c = self.sess.run( [self.optimizer, self.total_cost], \n",
    "                                     feed_dict= { self.x: minibatch_vectors, self.keep_rate_ph:self.keep_rate } )\n",
    "\n",
    "                # Compute average loss\n",
    "                avg_cost += c / (total_batch * self.batch_size)\n",
    "                \n",
    "            # Display some statistics about the step\n",
    "            if (epoch+1) % self.display_epoch_freq == 0:\n",
    "                print \"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \"Sample:\", self.sample()\n",
    "    \n",
    "    \n",
    "    def sample(self):\n",
    "        # This samples a sequence of tokens from the model starting with <S>.\n",
    "        # We only ever run the first timestep of the model, and use an effective batch size of one\n",
    "        # but we leave the model unrolled for multiple steps, and use the full batch size to simplify \n",
    "        # the training code. This slows things down.\n",
    "\n",
    "        def brittle_sampler():\n",
    "            # The main sampling code. Can fail randomly due to rounding errors that yield probibilities\n",
    "            # that don't sum to one.\n",
    "            \n",
    "            word_indices = [0]    # 0 here is the \"<S>\" symbol\n",
    "            for i in range(self.sequence_length - 1):\n",
    "                dummy_x = np.zeros((self.batch_size, self.sequence_length))\n",
    "                dummy_x[0][0] = word_indices[-1]\n",
    "                feed_dict = {self.x: dummy_x, self.keep_rate_ph:1.0}\n",
    "                if i > 0:\n",
    "                    feed_dict[self.h_zero] = h\n",
    "                    feed_dict[self.c_zero] = c           \n",
    "                h, c, logits = self.sess.run([self.h[1], self.c[1], self.logits[0]], \n",
    "                                             feed_dict=feed_dict)  \n",
    "                logits = logits[0, :]    # Discard all but first batch entry\n",
    "                exp_logits = np.exp(logits - np.max(logits))\n",
    "                distribution = exp_logits / exp_logits.sum()\n",
    "                sampled_index = np.flatnonzero(np.random.multinomial(1, distribution))[0]\n",
    "                word_indices.append(sampled_index)\n",
    "            words = [indices_to_words[index] for index in word_indices]\n",
    "            return ' '.join(words)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                sample = brittle_sampler()\n",
    "                return sample\n",
    "            except ValueError as e:  # Retry if we experience a random failure.\n",
    "                pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Training.\n",
      "Epoch: 1 Cost: 0.241791106535 Sample: <S> audience <UNK> might <UNK> <UNK> <UNK> <UNK> </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 2 Cost: 0.20780012147 Sample: <S> while <UNK> , <UNK> , works when feeling american to the <UNK> feels is come\n",
      "Epoch: 3 Cost: 0.200529943361 Sample: <S> wit , set the but <UNK> <UNK> funny this <UNK> which <UNK> <UNK> . </S>\n",
      "Epoch: 4 Cost: 0.196078305895 Sample: <S> all we <UNK> you good fans <UNK> yet a i <UNK> life in <UNK> <UNK>\n",
      "Epoch: 5 Cost: 0.193033874938 Sample: <S> the <UNK> , in a <UNK> make 's probably . <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 6 Cost: 0.190171249888 Sample: <S> worst <UNK> does in be melodrama <UNK> short much the <UNK> as obvious . </S>\n",
      "Epoch: 7 Cost: 0.188497461604 Sample: <S> the like this <UNK> <UNK> with need <UNK> just the film of this a <UNK>\n",
      "Epoch: 8 Cost: 0.186405470877 Sample: <S> worst ... a first <UNK> <UNK> <UNK> <UNK> of this than <UNK> i had you\n",
      "Epoch: 9 Cost: 0.18514988007 Sample: <S> n't neither ! 's <UNK> , not not <UNK> -rrb- <UNK> about the <UNK> <UNK>\n",
      "Epoch: 10 Cost: 0.183785191088 Sample: <S> but it 's <UNK> to little <UNK> , be first to though it your -lrb-\n",
      "Epoch: 11 Cost: 0.182739931074 Sample: <S> is in a <UNK> , the movie 's <UNK> the find least fact of <UNK>\n",
      "Epoch: 12 Cost: 0.181328658353 Sample: <S> the humor about <UNK> , worth a <UNK> and <UNK> with <UNK> drama , but\n",
      "Epoch: 13 Cost: 0.180584504297 Sample: <S> take 's <UNK> <UNK> <UNK> that 's <UNK> , <UNK> , too piece interest in\n",
      "Epoch: 14 Cost: 0.179682629578 Sample: <S> so <UNK> <UNK> as <UNK> 's <UNK> <UNK> <UNK> <UNK> little <UNK> <UNK> of been\n",
      "Epoch: 15 Cost: 0.179063400536 Sample: <S> it 's there does an sense of ca i an quite even so neither kids\n",
      "Epoch: 16 Cost: 0.177981768594 Sample: <S> would <UNK> good <UNK> , <UNK> piece <UNK> of `` <UNK> <UNK> , <UNK> ,\n",
      "Epoch: 17 Cost: 0.177447517713 Sample: <S> the movie 's <UNK> in <UNK> out <UNK> idea by it romantic of a little\n",
      "Epoch: 18 Cost: 0.176983556964 Sample: <S> it makes it 's <UNK> <UNK> <UNK> is it <UNK> the <UNK> <UNK> rare <UNK>\n",
      "Epoch: 19 Cost: 0.176237016013 Sample: <S> <UNK> family and what is its <UNK> as <UNK> <UNK> <UNK> of <UNK> of <UNK>\n",
      "Epoch: 20 Cost: 0.176063216545 Sample: <S> this very `` next as <UNK> <UNK> and <UNK> <UNK> ' drama , <UNK> <UNK>\n",
      "Epoch: 21 Cost: 0.175194381764 Sample: <S> an <UNK> version has an <UNK> 's not <UNK> without the entertaining is <UNK> .\n",
      "Epoch: 22 Cost: 0.174697055058 Sample: <S> , a <UNK> <UNK> up as <UNK> ... you one a <UNK> and ` <UNK>\n",
      "Epoch: 23 Cost: 0.174469707139 Sample: <S> it is <UNK> <UNK> , <UNK> <UNK> <UNK> for the filmmakers , <UNK> it is\n",
      "Epoch: 24 Cost: 0.173874052637 Sample: <S> a film makes nearly actors direction , you 're family of <UNK> and <UNK> as\n",
      "Epoch: 25 Cost: 0.173510882439 Sample: <S> the <UNK> <UNK> <UNK> -rrb- a <UNK> <UNK> like one of a effort , this\n",
      "Epoch: 26 Cost: 0.173490335996 Sample: <S> <UNK> <UNK> from the first <UNK> in a <UNK> nearly really <UNK> into like a\n",
      "Epoch: 27 Cost: 0.172948481007 Sample: <S> it 's scenes looks <UNK> <UNK> really a <UNK> of <UNK> in <UNK> here is\n",
      "Epoch: 28 Cost: 0.172719298439 Sample: <S> a <UNK> up , an visual power as it : a show like strong ,\n",
      "Epoch: 29 Cost: 0.172323146553 Sample: <S> <UNK> <UNK> <UNK> for much <UNK> does that love <UNK> , the <UNK> <UNK> <UNK>\n",
      "Epoch: 30 Cost: 0.172008747404 Sample: <S> minutes and as <UNK> and all to <UNK> is <UNK> you 's film about <UNK>\n",
      "Epoch: 31 Cost: 0.171841884653 Sample: <S> , ? 's not , but <UNK> may <UNK> has there are more had another\n",
      "Epoch: 32 Cost: 0.171610193271 Sample: <S> <UNK> <UNK> the <UNK> in <UNK> is not much each in a <UNK> <UNK> ,\n",
      "Epoch: 33 Cost: 0.171376448689 Sample: <S> a always acting or , it back about this but the film ... a -rrb-\n",
      "Epoch: 34 Cost: 0.170980084123 Sample: <S> there 's great movie to the film is kids is enough war is it <UNK>\n",
      "Epoch: 35 Cost: 0.170719929717 Sample: <S> <UNK> will a <UNK> <UNK> has it 's <UNK> than <UNK> , as -rrb- does\n",
      "Epoch: 36 Cost: 0.17041871358 Sample: <S> what 's never <UNK> for funny , the screenplay neither , <UNK> by often old\n",
      "Epoch: 37 Cost: 0.170586912921 Sample: <S> despite the same , <UNK> were humor , what <UNK> <UNK> the <UNK> <UNK> at\n",
      "Epoch: 38 Cost: 0.170210727688 Sample: <S> it 's <UNK> 's <UNK> entertaining is <UNK> , they can <UNK> , not ca\n",
      "Epoch: 39 Cost: 0.170194051934 Sample: <S> one is -rrb- why also not completely into its <UNK> in the <UNK> <UNK> and\n",
      "Epoch: 40 Cost: 0.169874584585 Sample: <S> this <UNK> , <UNK> , certainly as the <UNK> does just the first real performance\n",
      "Epoch: 41 Cost: 0.169792317983 Sample: <S> just the <UNK> more <UNK> intelligent as <UNK> <UNK> as <UNK> <UNK> and <UNK> <UNK>\n",
      "Epoch: 42 Cost: 0.16959883092 Sample: <S> a <UNK> <UNK> <UNK> <UNK> , there is a big smart interesting a funny of\n",
      "Epoch: 43 Cost: 0.169330610019 Sample: <S> <UNK> 's <UNK> romantic of <UNK> with those -rrb- more , he is one than\n",
      "Epoch: 44 Cost: 0.169139281367 Sample: <S> ... and everything is less as it is <UNK> , a <UNK> to <UNK> once\n",
      "Epoch: 45 Cost: 0.169196914543 Sample: <S> was surprisingly for <UNK> , it 's -rrb- makes they comedy too light to if\n",
      "Epoch: 46 Cost: 0.168834696213 Sample: <S> just like you you can keep it 's very nearly good in this movie are\n",
      "Epoch: 47 Cost: 0.168850757859 Sample: <S> to the smart of become film itself , but <UNK> plays , <UNK> is fun\n",
      "Epoch: 48 Cost: 0.168638792002 Sample: <S> the hard <UNK> but once it 's <UNK> <UNK> the story from the <UNK> and\n",
      "Epoch: 49 Cost: 0.168531024546 Sample: <S> you the need of a good <UNK> an <UNK> is <UNK> of <UNK> is n't\n",
      "Epoch: 50 Cost: 0.168316418926 Sample: <S> then , , an <UNK> see you <UNK> every <UNK> performance my <UNK> a <UNK>\n",
      "Epoch: 51 Cost: 0.168226349083 Sample: <S> what never <UNK> of a <UNK> <UNK> about the <UNK> is a <UNK> <UNK> ...\n",
      "Epoch: 52 Cost: 0.168149684866 Sample: <S> <UNK> are much too <UNK> is <UNK> and <UNK> <UNK> work of <UNK> , you\n",
      "Epoch: 53 Cost: 0.168233625817 Sample: <S> despite <UNK> minutes is when watching as <UNK> <UNK> does , a <UNK> <UNK> to\n",
      "Epoch: 54 Cost: 0.168050783602 Sample: <S> <UNK> actors is all when too everything interesting next and not just much like her\n",
      "Epoch: 55 Cost: 0.167676735105 Sample: <S> less <UNK> ! </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 56 Cost: 0.167890499036 Sample: <S> the art because you <UNK> <UNK> and those , <UNK> does n't <UNK> in a\n",
      "Epoch: 57 Cost: 0.167484620755 Sample: <S> them like as <UNK> is <UNK> to her is no <UNK> and the action <UNK>\n",
      "Epoch: 58 Cost: 0.167604980595 Sample: <S> <UNK> <UNK> in a <UNK> 's back me <UNK> and romantic still <UNK> thing is\n",
      "Epoch: 59 Cost: 0.167294727582 Sample: <S> long into some best time that it could be a <UNK> at its original that\n",
      "Epoch: 60 Cost: 0.16732666077 Sample: <S> <UNK> but it is be <UNK> <UNK> on the <UNK> <UNK> on <UNK> to the\n",
      "Epoch: 61 Cost: 0.167140558362 Sample: <S> it 's <UNK> <UNK> has the thing is <UNK> the movie who <UNK> the most\n",
      "Epoch: 62 Cost: 0.166921176694 Sample: <S> a <UNK> and ... you 're -lrb- this direction is not much <UNK> more too\n",
      "Epoch: 63 Cost: 0.166944105517 Sample: <S> nothing <UNK> the <UNK> when its <UNK> plot <UNK> or be very <UNK> or if\n",
      "Epoch: 64 Cost: 0.167065673254 Sample: <S> ... a last really movies in the film should be <UNK> <UNK> <UNK> . </S>\n",
      "Epoch: 65 Cost: 0.166959901651 Sample: <S> while it is a movie <UNK> <UNK> is as while <UNK> <UNK> <UNK> . </S>\n",
      "Epoch: 66 Cost: 0.166675312953 Sample: <S> this makes <UNK> <UNK> 's <UNK> , if a film 's <UNK> 's <UNK> <UNK>\n",
      "Epoch: 67 Cost: 0.1668007866 Sample: <S> <UNK> 's <UNK> around the film up it is an sense of a dark <UNK>\n",
      "Epoch: 68 Cost: 0.166451622139 Sample: <S> a more <UNK> our film <UNK> too <UNK> heart as true is a piece ,\n",
      "Epoch: 69 Cost: 0.166485942223 Sample: <S> a interesting , <UNK> , <UNK> <UNK> <UNK> 's film has a <UNK> , <UNK>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70 Cost: 0.166334387931 Sample: <S> charming in the such half moments matter under the film itself is <UNK> <UNK> human\n",
      "Epoch: 71 Cost: 0.166583486579 Sample: <S> a dark and smart , a <UNK> its film i <UNK> 're <UNK> is the\n",
      "Epoch: 72 Cost: 0.166238445676 Sample: <S> it 's ... is for i <UNK> is too original , then has <UNK> <UNK>\n",
      "Epoch: 73 Cost: 0.166063136675 Sample: <S> this <UNK> is n't enough than these film 's <UNK> 's <UNK> from the <UNK>\n",
      "Epoch: 74 Cost: 0.1661427012 Sample: <S> a <UNK> into point movies <UNK> has something <UNK> in a <UNK> <UNK> , <UNK>\n",
      "Epoch: 75 Cost: 0.166248863394 Sample: <S> too <UNK> <UNK> may <UNK> and <UNK> only a <UNK> are <UNK> <UNK> do n't\n",
      "Epoch: 76 Cost: 0.165936338179 Sample: <S> with <UNK> to take some of sometimes <UNK> -rrb- else is like those <UNK> <UNK>\n",
      "Epoch: 77 Cost: 0.165914332776 Sample: <S> this is i dialogue , <UNK> <UNK> and nothing also just is boring , -lrb-\n",
      "Epoch: 78 Cost: 0.165901827993 Sample: <S> turns to <UNK> and <UNK> any <UNK> the <UNK> while the film is the <UNK>\n",
      "Epoch: 79 Cost: 0.16572270881 Sample: <S> ... <UNK> are just <UNK> are <UNK> of <UNK> into the characters to new home\n",
      "Epoch: 80 Cost: 0.165843218565 Sample: <S> and beautifully so <UNK> and how have <UNK> entertainment , the film 's first home\n",
      "Epoch: 81 Cost: 0.165548139901 Sample: <S> <UNK> ... its <UNK> <UNK> ' <UNK> at the <UNK> and <UNK> of this <UNK>\n",
      "Epoch: 82 Cost: 0.165638839205 Sample: <S> the <UNK> and i had <UNK> <UNK> , <UNK> to nothing as you <UNK> and\n",
      "Epoch: 83 Cost: 0.165711249366 Sample: <S> -lrb- <UNK> <UNK> offers <UNK> in all the <UNK> need like the <UNK> , it\n",
      "Epoch: 84 Cost: 0.165292704196 Sample: <S> you 're <UNK> , <UNK> 's <UNK> the <UNK> <UNK> compelling <UNK> <UNK> <UNK> as\n",
      "Epoch: 85 Cost: 0.165443090778 Sample: <S> <UNK> <UNK> <UNK> <UNK> may be of its <UNK> story is more <UNK> again ,\n",
      "Epoch: 86 Cost: 0.165298584284 Sample: <S> what 's <UNK> is <UNK> is a sense of <UNK> was if enough . </S>\n",
      "Epoch: 87 Cost: 0.165202329556 Sample: <S> what is as a very then it comes a story , <UNK> and a <UNK>\n",
      "Epoch: 88 Cost: 0.165350999796 Sample: <S> the film seems <UNK> for a compelling of it 's <UNK> <UNK> 's <UNK> do\n",
      "Epoch: 89 Cost: 0.165296993021 Sample: <S> <UNK> is more than you ca never <UNK> , short of you 're the <UNK>\n",
      "Epoch: 90 Cost: 0.165125897888 Sample: <S> also <UNK> <UNK> <UNK> by a <UNK> as <UNK> like charming <UNK> <UNK> <UNK> from\n",
      "Epoch: 91 Cost: 0.165052557985 Sample: <S> a documentary is one of <UNK> nothing like all you be <UNK> is about <UNK>\n",
      "Epoch: 92 Cost: 0.165086389491 Sample: <S> ... but i ... a <UNK> <UNK> <UNK> is so much <UNK> of bad the\n",
      "Epoch: 93 Cost: 0.164963980064 Sample: <S> it 's most picture is more <UNK> and a <UNK> , i feels more movie\n",
      "Epoch: 94 Cost: 0.164796795357 Sample: <S> very most <UNK> the <UNK> director is <UNK> to the <UNK> <UNK> despite the <UNK>\n",
      "Epoch: 95 Cost: 0.164757631042 Sample: <S> such great <UNK> <UNK> , <UNK> has one of the best of <UNK> has been\n",
      "Epoch: 96 Cost: 0.164851495262 Sample: <S> this <UNK> <UNK> 's <UNK> <UNK> <UNK> <UNK> <UNK> and <UNK> his characters only the\n",
      "Epoch: 97 Cost: 0.164698039944 Sample: <S> like <UNK> is <UNK> it 's a <UNK> is never <UNK> <UNK> film as <UNK>\n",
      "Epoch: 98 Cost: 0.164761637648 Sample: <S> <UNK> does you <UNK> and <UNK> his cinema <UNK> to the <UNK> for those as\n",
      "Epoch: 99 Cost: 0.164909132954 Sample: <S> you did n't <UNK> to <UNK> a <UNK> will be more more than the filmmakers\n",
      "Epoch: 100 Cost: 0.164744228125 Sample: <S> i have <UNK> <UNK> to a history , the <UNK> does n't we <UNK> <UNK>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##Agan, this is for 15 words per sentence\n",
    "\n",
    "model = LanguageModel(len(word_indices), 16)\n",
    "model.train(training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> the performances has its <UNK> action is so <UNK> <UNK> , <UNK> a only <UNK>\n",
      "<S> <UNK> as <UNK> do <UNK> a <UNK> movie about the <UNK> high <UNK> makes also\n",
      "<S> if seems about <UNK> <UNK> , the film is nothing of it 's do n't\n",
      "<S> <UNK> with everything that makes its <UNK> could have it 's <UNK> so to modern\n",
      "<S> it 's most of a <UNK> <UNK> with the <UNK> mr. most ` <UNK> ,\n",
      "<S> it about <UNK> <UNK> proves ... there 's <UNK> to <UNK> <UNK> <UNK> <UNK> <UNK>\n"
     ]
    }
   ],
   "source": [
    "print model.sample()\n",
    "print model.sample()\n",
    "print model.sample()\n",
    "print model.sample()\n",
    "print model.sample()\n",
    "print model.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional notes about RNN and Deep Learning\n",
    "\n",
    "https://gigaom.com/2013/08/16/were-on-the-cusp-of-deep-learning-for-the-masses-you-can-thank-google-later/\n",
    "\"**Word2vec** is prepackaged deep-learning software released by google and designed to understand the **relationships between words** with no human guidance. Just input a textual data set and let underlying predictive models get to work learning. [Word2vec](https://opensource.googleblog.com/2013/08/learning-meaning-behind-words.html) uses distributed representations of text to capture similarities among concepts. For example, it understands that Paris and France are related the same way Berlin and Germany are (capital and country), and not the same way Madrid and Italy are. This chart shows how well it can learn the concept of capital cities, just by reading lots of news articles -- with no human supervision\"\n",
    "\n",
    "\n",
    "http://neuralnetworksanddeeplearning.com/chap5.html \n",
    "\" In general, neurons in the earlier layers learn much more slowly than neurons in later layers. And while we've seen this in just a single network, there are fundamental reasons why this happens in many neural networks. The phenomenon is known as the **vanishing gradient** problem. Why does the vanishing gradient problem occur? Are there ways we can avoid it? And how should we deal with it in training deep neural networks? In fact, we'll learn shortly that it's not inevitable, although the alternative is not very attractive, either: sometimes the gradient gets much larger in earlier layers! This is the exploding gradient problem, and it's not much better news than the vanishing gradient problem. More generally, it turns out that the gradient in deep neural networks is unstable, tending to either explode or vanish in earlier layers. This instability is a fundamental problem for gradient-based learning in deep neural networks. It's something we need to understand, and, if possible, take steps to address. \"\n",
    "\n",
    "http://link.springer.com/chapter/10.1007%2F11840817_26#page-1\n",
    "\" Introduced in Bengio et al. (1994), the **exploding gradients** problem refers to the large increase in the norm\n",
    "of the gradient during training. Such events are due to the explosion of the long term components, which can grow exponentially more than short term ones. The vanishing gradients problem refers to the opposite behaviour, when long term components go exponentially fast to norm 0, making it impossible for the model to learn correlation between temporally distant events. \n",
    "\n",
    "Using an **L1 or L2 penalty** on the recurrent weights can help with exploding gradients. Assuming weights are initialized to small values, the largest singular value λ1 of Wrec is probably smaller than 1. The L1/L2 term can ensure that during training λ1 stays smaller than 1, and in this regime gradients can not explode. This approach limits the model to single point attractor at the origin, where any information inserted in the model dies out exponentially fast. This prevents the model to learn generator networks, nor can it exhibit long term memory traces. \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
