{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM lanugage model\n",
    "\n",
    "The data used here is a treebank dataset for sentiment analysis avilable in [Stanford's website] (https://nlp.stanford.edu/sentiment/). The texts are basically movie reviews from [Rotten Tomatoes](rottentomatoes.com) originally collected and published by Pang and Lee (2005), but this dataset has labels for every syntactically plausible phrase in thousands of sentences. Half of sentences were considered positive and the other half negative and labels were extracted from longer movie review. \n",
    "\n",
    "The [Stanford Parser](https://nlp.stanford.edu/software/lex-parser.shtml) was used as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Import the data from txt files **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Prof Bowman Class exercise - Randomeseed used!!! (see below)\n",
    "\n",
    "data_home = 'data/trees'\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Positive and negative\n",
    "easy_label_map = {0:0, 1:0, 2:None, 3:1, 4:1}\n",
    "\n",
    "def load_sst_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)  # Strip out the parse information \n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "\n",
    "    random.seed(1)\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "     \n",
    "training_set = load_sst_data(data_home  + '/train.txt')\n",
    "dev_set = load_sst_data(data_home  + '/dev.txt')\n",
    "test_set = load_sst_data(data_home  + '/test.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of one review\n",
      "\n",
      "{'text': 'Beneath the uncanny , inevitable and seemingly shrewd facade of movie-biz farce ... lies a plot cobbled together from largely flat and uncreative moments .'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print \"Example of one review\\n\\n\", training_set[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Convert the data to index vectors**\n",
    "\n",
    "To simplify implementation I am using length of 15 words per sentence. I'll use the mark of the start of each sentence with < S>, end with </ S>, mark tokens after </S> with a special word symbol < PAD>, and out-of-vocabulary words with < UNK>, for unknown. \n",
    "\n",
    "This vocabulary is constructed based on common words (more than 35 times repeated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size 445\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def sentence_to_padded_index_sequence(datasets):\n",
    "    '''Annotates datasets with feature vectors.'''\n",
    "    \n",
    "    START = \"<S>\"\n",
    "    END = \"</S>\"\n",
    "    END_PADDING = \"<PAD>\"\n",
    "    UNKNOWN = \"<UNK>\"\n",
    "    SEQ_LEN = 15\n",
    "    \n",
    "    # Extract vocabulary\n",
    "    def tokenize(string):\n",
    "        return string.lower().split()\n",
    "    \n",
    "    word_counter = collections.Counter()\n",
    "    for example in datasets[0]:\n",
    "        word_counter.update(tokenize(example['text']))\n",
    "    \n",
    "    vocabulary = set([word for word in word_counter if word_counter[word] > 35])\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary = [START, END, END_PADDING, UNKNOWN] + vocabulary\n",
    "    print \"vocabulary size\", len(vocabulary)\n",
    "        \n",
    "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "    indices_to_words = {v: k for k, v in word_indices.items()}\n",
    "        \n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            example['index_sequence'] = np.zeros((SEQ_LEN), dtype=np.int32)\n",
    "            \n",
    "            token_sequence = [START] + tokenize(example['text']) + [END]\n",
    "            \n",
    "            for i in range(SEQ_LEN):\n",
    "                if i < len(token_sequence):\n",
    "                    if token_sequence[i] in word_indices:  ## Words outside vocab are those with counts less than 25\n",
    "                        index = word_indices[token_sequence[i]]\n",
    "                    else:\n",
    "                        index = word_indices[UNKNOWN]\n",
    "                else:\n",
    "                    index = word_indices[END_PADDING]  ## If there are no more words then <PAD> is the feature written\n",
    "                example['index_sequence'][i] = index\n",
    "    return indices_to_words, word_indices\n",
    "    \n",
    "indices_to_words, word_indices = sentence_to_padded_index_sequence([training_set, dev_set, test_set])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Beneath the uncanny , inevitable and seemingly shrewd facade of movie-biz farce ... lies a plot cobbled together from largely flat and uncreative moments .', 'index_sequence': array([  0,   3, 334,   3, 126,   3, 291,   3,   3,   3, 383,   3,   3,\n",
      "       350,   3], dtype=int32)}\n",
      "Words in the vocabulary = 445\n"
     ]
    }
   ],
   "source": [
    "print training_set[0]\n",
    "print \"Words in the vocabulary =\", len(word_indices)\n",
    "#print [(k,word_indices[k]) for k in sorted(word_indices,key=word_indices.__getitem__)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM Model **\n",
    "\n",
    "This model will have dropout on the non-recurrent connections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name pywrap_tensorflow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-41389fad42b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name pywrap_tensorflow"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    def __init__(self, vocab_size, sequence_length):\n",
    "        # Define the hyperparameters\n",
    "        self.learning_rate = 0.3  # Should be about right\n",
    "        self.training_epochs = 250   # How long to train for - chosen to fit within class time\n",
    "        self.display_epoch_freq = 1  # How often to test and print out statistics\n",
    "        self.dim = 32  # The dimension of the hidden state of the RNN\n",
    "        self.embedding_dim = 16  # The dimension of the learned word embeddings\n",
    "        self.batch_size = 256    # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n",
    "        self.vocab_size = vocab_size  # Defined by the file reader above\n",
    "        self.sequence_length = sequence_length  # Defined by the file reader above\n",
    "        self.keep_rate = 0.75  # Used in dropout (at training time only, not at sampling time)\n",
    "    \n",
    "        \n",
    "        #### Start main editable code block ####\n",
    "        \n",
    "        # TODO: Define the trained parameters.\n",
    "        self.E = tf.Variable(tf.random_normal([self.vocab_size, self.embedding_dim], stddev=0.1))\n",
    "        \n",
    "        self.W = {'f': tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1)),\n",
    "                  'i': tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1)),\n",
    "                  'C': tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1)),\n",
    "                  'o': tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1)),\n",
    "                  'cl': tf.Variable(tf.random_normal([self.dim, vocab_size], stddev=0.1))\n",
    "                  }\n",
    "        \n",
    "        self.b = {'f': tf.Variable(tf.random_normal([self.dim], stddev=0.1)),\n",
    "                  'i': tf.Variable(tf.random_normal([self.dim], stddev=0.1)),\n",
    "                  'C': tf.Variable(tf.random_normal([self.dim], stddev=0.1)),\n",
    "                  'o': tf.Variable(tf.random_normal([self.dim], stddev=0.1)),\n",
    "                  'cl': tf.Variable(tf.random_normal([vocab_size], stddev=0.1))\n",
    "                  }\n",
    "\n",
    "        # Define the input placeholder(s).\n",
    "        # I'll supply this one, since it's needed in sampling. Add any others you need.\n",
    "        self.x = tf.placeholder(tf.int32, [None, self.sequence_length])\n",
    "        self.x_slices = tf.split(1, self.sequence_length, self.x)\n",
    "        self.keep_rate_ph = tf.placeholder(tf.float32, [])\n",
    "        \n",
    "        # TODO: Build the rest of the LSTM LM! Your model should populate the following four python lists.\n",
    "        # self.logits should contain one [batch_size, vocab_size]-shaped TF tensor of logits \n",
    "        #   for each of the 20 steps of the model.\n",
    "        self.logits = []\n",
    "        # self.costs should contain one [batch_size]-shaped TF tensor of cross-entropy loss \n",
    "        #   values for each of the 20 steps of the model.\n",
    "        self.costs = []\n",
    "        # self.h and c should each start contain one [batch_size, dim]-shaped TF tensor of LSTM\n",
    "        #   activations for each of the 21 *states* of the model -- one tensor of zeros for the \n",
    "        #   starting state followed by one tensor each for the remaining 20 steps.\n",
    "        self.h_zero = tf.zeros( [self.batch_size, self.dim] ) \n",
    "        self.c_zero = tf.zeros( [self.batch_size, self.dim] ) \n",
    "        self.h = []\n",
    "        self.c =[]\n",
    "                \n",
    "        # Define one step of the RNN for each word-place (index)\n",
    "        def step(x, h_prev, c_prev):\n",
    "            emb_x = tf.nn.embedding_lookup(self.E, x)\n",
    "            emb_x = tf.nn.dropout(emb_x, self.keep_rate_ph)\n",
    "            emb_h_prev = tf.concat(1, [emb_x, h_prev])\n",
    "            f_t = tf.nn.sigmoid( tf.matmul(emb_h_prev, self.W['f'])  + self.b['f'] )\n",
    "            i_t = tf.nn.sigmoid( tf.matmul(emb_h_prev, self.W['i'])  + self.b['i'] )\n",
    "            C_tilde_t = tf.nn.tanh( tf.matmul(emb_h_prev, self.W['C'])  + self.b['C'])\n",
    "            C_t = tf.mul( f_t, c_prev) +  tf.mul( i_t, C_tilde_t)\n",
    "            o_t = tf.nn.sigmoid( tf.matmul(emb_h_prev, self.W['o'])  + self.b['o'] )\n",
    "            h = o_t * tf.nn.tanh( C_t )\n",
    "            return h, C_t\n",
    "        \n",
    "        h_prev = self.h_zero\n",
    "        c_prev = self.c_zero\n",
    "        \n",
    "        for t in range(self.sequence_length-1):\n",
    "            x_t = tf.reshape(self.x_slices[t], [-1])\n",
    "            h_prev, c_prev = step(x_t, h_prev, c_prev)\n",
    "            self.logit_t = tf.matmul(tf.nn.dropout(h_prev, self.keep_rate_ph), self.W['cl']) + self.b['cl']\n",
    "            self.x_t_new = tf.reshape(self.x_slices[t+1], [-1])\n",
    "            self.logits.append( self.logit_t )\n",
    "            self.costs.append(tf.nn.sparse_softmax_cross_entropy_with_logits(self.logit_t, self.x_t_new ))\n",
    "            self.h.append(h_prev)\n",
    "            self.c.append(c_prev)\n",
    "            \n",
    "        \n",
    "        #### End main editable code block ####\n",
    "        \n",
    "        \n",
    "        #Sum costs for each word in each example, but average cost across examples.\n",
    "        self.costs_tensor = tf.concat(1, [tf.expand_dims(cost, 1) for cost in self.costs])\n",
    "        self.cost_per_example = tf.reduce_sum(self.costs_tensor, 1)\n",
    "        self.total_cost = tf.reduce_mean(self.cost_per_example)\n",
    "            \n",
    "        # This library call performs the main SGD update equation\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.total_cost)\n",
    "        \n",
    "        # Create an operation to fill zero values in for W and b\n",
    "        self.init = tf.initialize_all_variables()\n",
    "        \n",
    "        # Create a placeholder for the session that will be shared between training and evaluation\n",
    "        self.sess = None\n",
    "        \n",
    "                \n",
    "    def train(self, training_data):\n",
    "        \n",
    "        def get_minibatch(dataset, start_index, end_index):\n",
    "            indices = range(start_index, end_index)\n",
    "            vectors = np.vstack([dataset[i]['index_sequence'] for i in indices])\n",
    "            return vectors\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.sess.run(self.init)\n",
    "        print 'Training.'\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(self.training_epochs):\n",
    "            random.shuffle(training_set)\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(len(training_set) / self.batch_size)\n",
    "            \n",
    "            # Loop over all batches in epoch\n",
    "            for i in range(total_batch):\n",
    "                # Assemble a minibatch of the next B examples: we ask for the indeex SEQUENCE OF EACH OBSERVATION\n",
    "                # BASICALLY, THE VECTORS WITH THE NUMBERS (INDEX TO WORDS)\n",
    "                minibatch_vectors = get_minibatch(training_set, self.batch_size * i, self.batch_size * (i + 1))\n",
    "\n",
    "                # Run the optimizer to take a gradient step, and also fetch the value of the \n",
    "                # cost function for logging\n",
    "                \n",
    "                # Example of how I PRINT actual values\n",
    "                #xslices,x_new, logit_t = self.sess.run([self.x_slices, self.x_t_new, self.logit_t ],\n",
    "                #                                       feed_dict={self.x: minibatch_vectors})\n",
    "                #print \"x slices\", xslices[0], \"x new\", x_new, \"logit\", logit_t\n",
    "\n",
    "                _, c = self.sess.run([self.optimizer, self.total_cost], \n",
    "                                     feed_dict={self.x: minibatch_vectors,self.keep_rate_ph:self.keep_rate})\n",
    "\n",
    "                # Compute average loss\n",
    "                avg_cost += c / (total_batch * self.batch_size)\n",
    "                \n",
    "            # Display some statistics about the step\n",
    "            if (epoch+1) % self.display_epoch_freq == 0:\n",
    "                print \"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \"Sample:\", self.sample()\n",
    "    \n",
    "    \n",
    "    def sample(self):\n",
    "        # This samples a sequence of tokens from the model starting with <S>.\n",
    "        # We only ever run the first timestep of the model, and use an effective batch size of one\n",
    "        # but we leave the model unrolled for multiple steps, and use the full batch size to simplify \n",
    "        # the training code. This slows things down.\n",
    "\n",
    "        def brittle_sampler():\n",
    "            # The main sampling code. Can fail randomly due to rounding errors that yield probibilities\n",
    "            # that don't sum to one.\n",
    "            \n",
    "            word_indices = [0]    # 0 here is the \"<S>\" symbol\n",
    "            for i in range(self.sequence_length - 1):\n",
    "                dummy_x = np.zeros((self.batch_size, self.sequence_length))\n",
    "                dummy_x[0][0] = word_indices[-1]\n",
    "                feed_dict = {self.x: dummy_x, self.keep_rate_ph:1.0}\n",
    "                if i > 0:\n",
    "                    feed_dict[self.h_zero] = h\n",
    "                    feed_dict[self.c_zero] = c           \n",
    "                h, c, logits = self.sess.run([self.h[1], self.c[1], self.logits[0]], \n",
    "                                             feed_dict=feed_dict)  \n",
    "                logits = logits[0, :]    # Discard all but first batch entry\n",
    "                exp_logits = np.exp(logits - np.max(logits))\n",
    "                distribution = exp_logits / exp_logits.sum()\n",
    "                sampled_index = np.flatnonzero(np.random.multinomial(1, distribution))[0]\n",
    "                word_indices.append(sampled_index)\n",
    "            words = [indices_to_words[index] for index in word_indices]\n",
    "            return ' '.join(words)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                sample = brittle_sampler()\n",
    "                return sample\n",
    "            except ValueError as e:  # Retry if we experience a random failure.\n",
    "                pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e5a7b0530515>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLanguageModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-037566f73e5c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_size, sequence_length)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# TODO: Define the trained parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         self.W = {'f': tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1)),\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "model = LanguageModel(len(word_indices), 21)\n",
    "model.train(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print model.sample()\n",
    "print model.sample()\n",
    "print model.sample()\n",
    "print model.sample()\n",
    "print model.sample()\n",
    "print model.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional notes about RNN\n",
    "\n",
    "http://neuralnetworksanddeeplearning.com/chap5.html \n",
    "In general, neurons in the earlier layers learn much more slowly than neurons in later layers. And while we've seen this in just a single network, there are fundamental reasons why this happens in many neural networks. The phenomenon is known as the vanishing gradient problem\n",
    " Why does the vanishing gradient problem occur? Are there ways we can avoid it? And how should we deal with it in training deep neural networks? In fact, we'll learn shortly that it's not inevitable, although the alternative is not very attractive, either: sometimes the gradient gets much larger in earlier layers! This is the exploding gradient problem, and it's not much better news than the vanishing gradient problem. More generally, it turns out that the gradient in deep neural networks is unstable, tending to either explode or vanish in earlier layers. This instability is a fundamental problem for gradient-based learning in deep neural networks. It's something we need to understand, and, if possible, take steps to address.\n",
    "\n",
    "http://link.springer.com/chapter/10.1007%2F11840817_26#page-1\n",
    "Introduced in Bengio et al. (1994), the exploding gradients problem refers to the large increase in the norm\n",
    "of the gradient during training. Such events are due to the explosion of the long term components, which can grow exponentially more than short term ones. The vanishing gradients problem refers to the opposite behaviour, when long term components go exponentially fast to norm 0, making it impossible for the model to learn correlation between temporally distant events. \n",
    "\n",
    "Using an L1 or L2 penalty on the recurrent weights can help with exploding gradients. Assuming weights are initialized to small values, the largest singular value λ1 of Wrec is probably smaller than 1. The L1/L2 term can ensure that during training λ1 stays smaller than 1, and in this regime gradients can not explode. This approach limits the model to single point attractor at the origin, where any information inserted in the model dies out exponentially fast. This prevents the model to learn generator networks, nor can it exhibit long term memory traces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
